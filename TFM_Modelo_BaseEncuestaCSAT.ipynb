{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatusilva/test/blob/master/TFM_Modelo_BaseEncuestaCSAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21ebfc5",
      "metadata": {
        "id": "c21ebfc5"
      },
      "source": [
        "# TFM — CSAT (Notebook comentado)\n",
        "\n",
        "Este cuaderno ha sido **anotado** para que el flujo quede claro para lectores y tutores.\n",
        "- Secciones: *Importaciones, Montaje de Drive, Carga de datos, Limpieza, Ingeniería, Partición/Tuning, Entrenamiento, Evaluación, SHAP, Exportes*.\n",
        "- Los comentarios `# === ... ===` **no alteran** la ejecución; solo documentan el propósito de cada bloque.\n",
        "- Recomendación: ejecutar **Runtime → Restart & run all** para reproducibilidad.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03cb9617",
      "metadata": {
        "id": "03cb9617"
      },
      "source": [
        "# Trabajo Fin de Máster (TFM)\n",
        "## Predicción de la satisfacción del cliente (CSAT) en un Contact Center\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffff4779",
      "metadata": {
        "id": "ffff4779"
      },
      "outputs": [],
      "source": [
        "# === Bloque misceláneo ===\n",
        "!pip -q install xgboost imbalanced-learn\n",
        "!wget https://github.com/fatusilva/test/raw/master/BaseEncuestasClientes.xlsx -O archivo.xlsx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c64a47",
      "metadata": {
        "id": "55c64a47"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "import numpy as np, json\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "#from google.colab import files\n",
        "\n",
        "# Modelado / Métricas\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score,make_scorer, f1_score, recall_score,precision_score\n",
        "\n",
        "# Opcional (plots)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "pd.set_option('display.max_colwidth', 120)\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "# ---- Helper: normalizar nombres de columnas y mapear a canónicos ----\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00b0ba4",
      "metadata": {
        "id": "b00b0ba4"
      },
      "outputs": [],
      "source": [
        "# === Carga de datos: lee fuentes (CSV/Excel/Drive) al DataFrame ===\n",
        "FILE_PATH = \"https://github.com/fatusilva/test/raw/master/BaseEncuestasClientes.xlsx\"\n",
        "SHEET_NAME = None\n",
        "\n",
        "# Ver qué hojas tiene el archivo\n",
        "xl = pd.ExcelFile(FILE_PATH)\n",
        "print(\"Hojas encontradas:\", xl.sheet_names)\n",
        "\n",
        "# Cargar la hoja (si SHEET_NAME es None usa la primera)\n",
        "df = pd.read_excel(FILE_PATH, sheet_name=SHEET_NAME or 0, header=1)\n",
        "\n",
        "# Normalizar nombres\n",
        "df.columns = [str(c).strip() for c in df.columns]\n",
        "\n",
        "print(\"Shape inicial:\", df.shape)\n",
        "print(\"Primeras columnas:\", df.columns[:10].tolist())\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878e42a7",
      "metadata": {
        "id": "878e42a7"
      },
      "outputs": [],
      "source": [
        "# === Parsers robustos (reemplaza la celda de parsers anterior) ===\n",
        "import re, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "# Nombres que usás (confirmados):\n",
        "CSAT_COL    = \"CSAT General\"\n",
        "CANAL_COL   = \"Origin_sf\"\n",
        "MOTIVO_COL   = \"motivo_contacto\"\n",
        "PROD_COL    = \"Producto_sf\"\n",
        "SUBPROD_COL = \"Subproduct_sf\"   # si no existe, lo detectamos y usamos PROD_COL\n",
        "DUR_COL     = \"Duración (en segundos)\"\n",
        "\n",
        "def _norm(s):\n",
        "    # normaliza para matching tolerante (espacios invisibles, acentos, may/min)\n",
        "    import unicodedata\n",
        "    s = str(s)\n",
        "    s = s.replace(\"\\xa0\",\" \").strip()\n",
        "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
        "    return s.lower()\n",
        "\n",
        "# Mapa de nombres normalizados -> nombre real en df\n",
        "norm_map = {_norm(c): c for c in df.columns}\n",
        "\n",
        "def col(df, wanted):\n",
        "    # busca una columna por nombre exacto, tolerando acentos/espacios/case\n",
        "    if wanted in df.columns:\n",
        "        return df[wanted], wanted\n",
        "    key = _norm(wanted)\n",
        "    if key in norm_map:\n",
        "        real = norm_map[key]\n",
        "        return df[real], real\n",
        "    # fallback: intentar coincidencia por inicio (útil si hay sufijos raros)\n",
        "    for k, real in norm_map.items():\n",
        "        if k == key or k.startswith(key):\n",
        "            return df[real], real\n",
        "    raise KeyError(f\"No encuentro la columna '{wanted}'. Revisa tildes/espacios. \"\n",
        "                   f\"Ejemplos en df.columns: {list(df.columns)[:8]}\")\n",
        "\n",
        "# Construimos df_eda sin hacer selección por lista (evita KeyError por índices)\n",
        "df_eda = pd.DataFrame(index=df.index)\n",
        "\n",
        "# CSAT\n",
        "s_csat, CSAT_REAL = col(df, CSAT_COL)\n",
        "# Duración\n",
        "try:\n",
        "    s_dur, DUR_REAL = col(df, DUR_COL)\n",
        "except KeyError:\n",
        "    s_dur, DUR_REAL = pd.Series(np.nan, index=df.index), None\n",
        "# Canal\n",
        "s_canal, CANAL_REAL = col(df, CANAL_COL)\n",
        "# Motivo\n",
        "s_motivo, MOTIVO_REAL = col(df, MOTIVO_COL)\n",
        "# Producto\n",
        "s_prod, PROD_REAL = col(df, PROD_COL)\n",
        "# Subproducto (si no está, usamos producto)\n",
        "try:\n",
        "    s_subp, SUBPROD_REAL = col(df, SUBPROD_COL)\n",
        "except KeyError:\n",
        "    s_subp, SUBPROD_REAL = s_prod, PROD_REAL\n",
        "\n",
        "# Copiamos columnas “reales” (como están en tu df) y generamos auxiliares\n",
        "df_eda[CSAT_REAL]   = s_csat\n",
        "df_eda[CANAL_REAL]  = s_canal\n",
        "df_eda[MOTIVO_REAL]  = s_motivo\n",
        "df_eda[PROD_REAL]   = s_prod\n",
        "df_eda[SUBPROD_REAL]= s_subp\n",
        "if DUR_REAL is not None:\n",
        "    df_eda[DUR_REAL] = s_dur\n",
        "\n",
        "# --- Parsers ---\n",
        "def parse_csat_to_1_5(s):\n",
        "    s = s.astype(str).str.strip().str.lower()\n",
        "\n",
        "    # Mapeo por texto (ajustá etiquetas si usás otras)\n",
        "    mapa = {\n",
        "        \"muy insatisfecho\": 1, \"insatisfecho\": 2, \"neutral\": 3,\n",
        "        \"satisfecho\": 4, \"muy satisfecho\": 5,\n",
        "        \"muy malo\": 1, \"malo\": 2, \"regular\": 3, \"bueno\": 4, \"muy bueno\": 5\n",
        "    }\n",
        "    out = s.map(mapa)\n",
        "\n",
        "    # Para lo que no mapeó, intento extraer un número 1..5 (con punto o coma)\n",
        "    mask_na = out.isna()\n",
        "    if mask_na.any():\n",
        "        s_na = s[mask_na].str.replace(\",\", \".\", regex=False)\n",
        "        # Captura SOLO el número (un único grupo) → devuelve una Serie, no un DF\n",
        "        num = s_na.str.extract(r'([1-5](?:\\.\\d+)?)', expand=False)\n",
        "        out.loc[mask_na] = pd.to_numeric(num, errors='coerce')\n",
        "\n",
        "    # Redondeo y recorte al rango [1,5]\n",
        "    out = pd.to_numeric(out, errors='coerce').round(0).clip(1, 5)\n",
        "    return out\n",
        "\n",
        "\n",
        "def parse_seconds(col):\n",
        "    s = col.astype(str).str.strip()\n",
        "    def to_sec_one(x):\n",
        "        if not x or x.lower() in {\"nan\",\"none\"}: return np.nan\n",
        "        if \":\" in x:\n",
        "            x2 = x.replace(\",\", \".\")\n",
        "            parts = x2.split(\":\")\n",
        "            try:\n",
        "                parts = [float(p) for p in parts]\n",
        "            except:\n",
        "                return np.nan\n",
        "            if len(parts) == 3:\n",
        "                h,m,ss = parts; return h*3600 + m*60 + ss\n",
        "            if len(parts) == 2:\n",
        "                m,ss = parts;  return m*60 + ss\n",
        "            return parts[0]\n",
        "        try:\n",
        "            return float(x.replace(\",\", \".\"))\n",
        "        except:\n",
        "            return np.nan\n",
        "    return s.apply(to_sec_one)\n",
        "\n",
        "# --- Auxiliares ---\n",
        "df_eda[\"_csat_num\"] = parse_csat_to_1_5(df_eda[CSAT_REAL])\n",
        "\n",
        "if DUR_REAL is not None:\n",
        "    df_eda[\"_dur_num\"] = parse_seconds(df_eda[DUR_REAL])\n",
        "else:\n",
        "    df_eda[\"_dur_num\"] = np.nan\n",
        "\n",
        "# CSAT binario: 0 = <=3, 1 = >=4\n",
        "df_eda[\"csat_bin\"] = np.where(df_eda[\"_csat_num\"] >= 4, 1, 0)\n",
        "\n",
        "print(\"Columnas reales usadas ->\",\n",
        "      f\"CSAT: {CSAT_REAL} | CANAL: {CANAL_REAL} | MOTIVO: {MOTIVO_REAL}| PROD: {PROD_REAL} | SUBPROD: {SUBPROD_REAL} | DUR: {DUR_REAL}\")\n",
        "print(\"Preview CSAT únicos:\", sorted(pd.Series(df_eda[\"_csat_num\"].dropna().unique()).tolist())[:10])\n",
        "\n",
        "if DUR_REAL is not None:\n",
        "    print(\"Duración (s) min/med/max:\",\n",
        "          df_eda[\"_dur_num\"].min(skipna=True),\n",
        "          df_eda[\"_dur_num\"].median(skipna=True),\n",
        "          df_eda[\"_dur_num\"].max(skipna=True))\n",
        "else:\n",
        "    print(\"Duración no encontrada; Fig. 5 puede omitirse.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HHqMsFwwnQRQ",
      "metadata": {
        "id": "HHqMsFwwnQRQ"
      },
      "source": [
        "## 2.3 Análisis Exploratorio de Datos (EDA) Visual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d33f8b",
      "metadata": {
        "id": "60d33f8b"
      },
      "outputs": [],
      "source": [
        "# === EDA Visual: genera Fig3/4/5 y redacta \"lecturas\" sugeridas ===\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "# Asume que ya existen: df_eda, CSAT_REAL, CANAL_REAL, PROD_REAL, SUBPROD_REAL, DUR_REAL\n",
        "# y que df_eda tiene _csat_num, csat_bin, _dur_num (creados en la celda de parsers)\n",
        "\n",
        "lecturas = []\n",
        "\n",
        "# ---------------- FIGURA 3: CSAT por canal ----------------\n",
        "df_canal = df_eda[[CANAL_REAL, \"_csat_num\", \"csat_bin\"]].dropna(subset=[CANAL_REAL, \"_csat_num\"]).copy()\n",
        "msg3 = []\n",
        "\n",
        "# Boxplot si hay ≥2 canales con ≥5 obs\n",
        "groups, labels = [], []\n",
        "for canal, sub in df_canal.groupby(CANAL_REAL):\n",
        "    vals = sub[\"_csat_num\"].dropna().values\n",
        "    if len(vals) >= 5:\n",
        "        groups.append(vals); labels.append(str(canal))\n",
        "\n",
        "if len(groups) >= 2:\n",
        "    fig = plt.figure(figsize=(9,5))\n",
        "    plt.boxplot(groups, labels=labels, showfliers=False)\n",
        "    plt.title(\"CSAT (1–5) por Canal\")\n",
        "    plt.xlabel(\"Canal\"); plt.ylabel(\"CSAT (1–5)\")\n",
        "    plt.xticks(rotation=15, ha=\"right\")\n",
        "    plt.tight_layout(); plt.savefig(\"fig3_csat_por_canal.png\", dpi=120); plt.close(fig)\n",
        "    print(\"✅ fig3_csat_por_canal.png\")\n",
        "else:\n",
        "    print(\"ℹ️ Boxplot omitido por bajo volumen por canal.\")\n",
        "\n",
        "# Barras: % insatisfechos (csat_bin=0) filtrando canales con ≥30 encuestas\n",
        "grp = df_canal.groupby(CANAL_REAL)\n",
        "counts = grp[\"csat_bin\"].size()\n",
        "mask = counts >= 30\n",
        "rate = pd.Series(dtype=float)\n",
        "if mask.any():\n",
        "    rate = (grp[\"csat_bin\"].apply(lambda s: (s==0).mean())[mask]).sort_values(ascending=False)\n",
        "    fig = plt.figure(figsize=(9,5))\n",
        "    rate.plot(kind=\"bar\")\n",
        "    plt.title(\"Tasa de insatisfacción por Canal (csat_bin=0) [≥30 encuestas]\")\n",
        "    plt.xlabel(\"Canal\"); plt.ylabel(\"Porcentaje\")\n",
        "    plt.xticks(rotation=15, ha=\"right\")\n",
        "    plt.tight_layout(); plt.savefig(\"fig3b_tasa_insatisfaccion_por_canal.png\", dpi=120); plt.close(fig)\n",
        "    print(\"✅ fig3b_tasa_insatisfaccion_por_canal.png\")\n",
        "else:\n",
        "    # Fallback: CSAT medio por canal (si no hay volumen para tasas)\n",
        "    mean_by_c = grp[\"_csat_num\"].mean().sort_values(ascending=True)\n",
        "    if len(mean_by_c) > 0:\n",
        "        fig = plt.figure(figsize=(9,5))\n",
        "        mean_by_c.plot(kind=\"barh\")\n",
        "        plt.title(\"CSAT medio por Canal (fallback por bajo volumen)\")\n",
        "        plt.xlabel(\"CSAT (1–5)\"); plt.ylabel(\"Canal\")\n",
        "        plt.tight_layout(); plt.savefig(\"fig3_fallback_csat_medio_por_canal.png\", dpi=120); plt.close(fig)\n",
        "        print(\"✅ fig3_fallback_csat_medio_por_canal.png\")\n",
        "\n",
        "# Lectura sugerida (se arma con mediana y con tasa si hubo)\n",
        "medianas = grp[\"_csat_num\"].median().sort_values()\n",
        "if len(medianas) >= 1:\n",
        "    worst_c = medianas.index[0]; worst_med = medianas.iloc[0]\n",
        "    best_c  = medianas.index[-1]; best_med  = medianas.iloc[-1]\n",
        "    msg3.append(f\"Diferencias de distribución por canal: la mediana más baja de CSAT se observa en **{worst_c}** (≈ {worst_med:.2f}), mientras que la más alta en **{best_c}** (≈ {best_med:.2f}).\")\n",
        "\n",
        "if len(rate) > 0:\n",
        "    top_r = rate.index[0]; top_rv = rate.iloc[0]; n_top = int(counts[top_r])\n",
        "    msg3.append(f\"En términos de tasa histórica de insatisfacción, **{top_r}** presenta el valor más alto (≈ {top_rv:.0%}, n={n_top}). Esto orienta acciones focalizadas por canal.\")\n",
        "else:\n",
        "    msg3.append(\"Por bajo volumen en algunos canales, se reporta **CSAT medio por canal** como referencia.\")\n",
        "\n",
        "lecturas.append((\"Figura 3 — CSAT por canal\", \" \".join(msg3)))\n",
        "\n",
        "# ---------------- FIGURA 4: CSAT por producto/subproducto ----------------\n",
        "cat_col = SUBPROD_REAL if SUBPROD_REAL in df_eda.columns else PROD_REAL\n",
        "df_cat = df_eda[[cat_col, \"csat_bin\"]].dropna().copy()\n",
        "\n",
        "min_n = 50  # umbral de volumen por categoría (ajustable)\n",
        "counts_cat = df_cat[cat_col].value_counts()\n",
        "keep = counts_cat[counts_cat >= min_n].index\n",
        "\n",
        "msg4 = []\n",
        "if len(keep) > 0:\n",
        "    df_top = df_cat[df_cat[cat_col].isin(keep)].copy()\n",
        "    rate_cat = df_top.groupby(cat_col)[\"csat_bin\"].apply(lambda s: (s==0).mean()).sort_values(ascending=False)\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    rate_cat.plot(kind=\"bar\")\n",
        "    plt.title(f\"Tasa de insatisfacción por {cat_col} [≥{min_n} encuestas]\")\n",
        "    plt.xlabel(cat_col); plt.ylabel(\"Porcentaje (csat_bin=0)\")\n",
        "    plt.xticks(rotation=30, ha=\"right\")\n",
        "    plt.tight_layout(); plt.savefig(\"fig4_csat_por_producto_subproducto.png\", dpi=120); plt.close(fig)\n",
        "    print(\"✅ fig4_csat_por_producto_subproducto.png\")\n",
        "\n",
        "    # Top 3 para redactar\n",
        "    topN = rate_cat.head(3)\n",
        "    toplist = [f\"{idx} ({val:.0%})\" for idx, val in topN.items()]\n",
        "    msg4.append(\"Se evidencian categorías con **tasa histórica** mayor de insatisfacción: \" + \", \".join(toplist) + \".\")\n",
        "    msg4.append(\"Esto justifica el uso de **target mean/frequency encoding** a nivel de producto/subproducto, que luego emerge como relevante en SHAP.\")\n",
        "else:\n",
        "    msg4.append(f\"Omitido por volumen bajo: ninguna categoría con ≥{min_n} encuestas. Si necesitás graficar, bajá el umbral a 30.\")\n",
        "\n",
        "lecturas.append((\"Figura 4 — CSAT por producto/subproducto\", \" \".join(msg4)))\n",
        "\n",
        "# ---------------- FIGURA 5: Duración (s) vs CSAT ----------------\n",
        "msg5 = []\n",
        "if DUR_REAL is not None:\n",
        "    dur = df_eda[[\"_dur_num\", \"_csat_num\", \"csat_bin\"]].dropna().copy()\n",
        "    if len(dur) >= 50:\n",
        "        # Recorte de outliers (p99.5)\n",
        "        p995 = dur[\"_dur_num\"].quantile(0.995)\n",
        "        dur = dur[dur[\"_dur_num\"] <= p995]\n",
        "\n",
        "        dur[\"dur_bin\"] = pd.qcut(dur[\"_dur_num\"], q=10, duplicates=\"drop\")\n",
        "        summary = dur.groupby(\"dur_bin\").agg(\n",
        "            csat_mean=(\"_csat_num\", \"mean\"),\n",
        "            insat_rate=(\"csat_bin\", lambda s: (s==0).mean()),\n",
        "            dur_med=(\"_dur_num\", \"median\")\n",
        "        ).reset_index().sort_values(\"dur_med\")\n",
        "\n",
        "        # plot\n",
        "        fig = plt.figure(figsize=(9,5))\n",
        "        ax = plt.gca()\n",
        "        ax.plot(summary[\"dur_med\"], summary[\"csat_mean\"], marker=\"o\", label=\"CSAT medio (1–5)\")\n",
        "        ax.set_xlabel(\"Duración (s) - mediana por decil\"); ax.set_ylabel(\"CSAT medio (1–5)\")\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(summary[\"dur_med\"], summary[\"insat_rate\"], marker=\"s\", linestyle=\"--\", label=\"Tasa insatisfacción\")\n",
        "        ax2.set_ylabel(\"Tasa insatisfacción (0–1)\")\n",
        "        plt.title(\"Relación Duración (s) – CSAT / Tasa de Insatisfacción\")\n",
        "        lines, labels_ = ax.get_legend_handles_labels()\n",
        "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "        ax2.legend(lines + lines2, labels_ + labels2, loc=\"best\")\n",
        "        plt.tight_layout(); plt.savefig(\"fig5_duracion_vs_csat.png\", dpi=120); plt.close(fig)\n",
        "        print(\"✅ fig5_duracion_vs_csat.png\")\n",
        "\n",
        "        # lectura cuantitativa\n",
        "        first = summary.iloc[0]; last = summary.iloc[-1]\n",
        "        delta = last[\"csat_mean\"] - first[\"csat_mean\"]\n",
        "        msg5.append(f\"**Duraciones muy cortas** (≈ {first['dur_med']:.0f}s) muestran CSAT medio ≈ {first['csat_mean']:.2f};\")\n",
        "        msg5.append(f\"a medida que la duración crece (≈ {last['dur_med']:.0f}s), el CSAT medio sube a ≈ {last['csat_mean']:.2f} (Δ ≈ {delta:.2f}).\")\n",
        "        msg5.append(\"La hipótesis es que mayor tiempo efectivo permite resolver mejor, consistente con la importancia de **Duración (s)** observada en SHAP.\")\n",
        "    else:\n",
        "        msg5.append(\"Omitido: no hay suficientes datos no nulos para duración (≥50).\")\n",
        "else:\n",
        "    msg5.append(\"No se encontró la columna de Duración; esta figura puede omitirse.\")\n",
        "\n",
        "lecturas.append((\"Figura 5 — Duración (s) vs CSAT\", \" \".join(msg5)))\n",
        "\n",
        "# ---------------- Mostrar \"lecturas\" listas para pegar ----------------\n",
        "print(\"\\n\\n==== LECTURAS SUGERIDAS (copiar/pegar bajo cada figura en el TFM) ====\\n\")\n",
        "for titulo, texto in lecturas:\n",
        "    print(f\"{titulo}\\n{texto}\\n\".replace(\"{text}\", texto))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explicabilidad con SHAP"
      ],
      "metadata": {
        "id": "TmGKnzP6GsrG"
      },
      "id": "TmGKnzP6GsrG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7cff29b",
      "metadata": {
        "id": "a7cff29b"
      },
      "outputs": [],
      "source": [
        "# === Explicabilidad con SHAP ===\n",
        "# Importancia global (bar/beeswarm) y dependence plots para variables top.\n",
        "# Detectar y convertir columnas de fechas si existen\n",
        "posibles_fecha_encuesta = ['Fecha registrada', 'Fecha Registrada']\n",
        "fecha_encuesta_col = next((c for c in posibles_fecha_encuesta if c in df.columns), None)\n",
        "\n",
        "if fecha_encuesta_col:\n",
        "    df[fecha_encuesta_col] = pd.to_datetime(df[fecha_encuesta_col], errors='coerce')\n",
        "\n",
        "# cuenta_creada (alta del cliente)\n",
        "if 'cuenta_creada' in df.columns:\n",
        "    df['cuenta_creada'] = pd.to_datetime(df['cuenta_creada'], errors='coerce')\n",
        "\n",
        "# Antigüedad del cliente en meses (aprox; días/30.44)\n",
        "if fecha_encuesta_col and 'cuenta_creada' in df.columns:\n",
        "    diff_days = (df[fecha_encuesta_col] - df['cuenta_creada']).dt.days\n",
        "    df['antiguedad_cliente_meses'] = (diff_days / 30.44).clip(lower=0)\n",
        "else:\n",
        "    df['antiguedad_cliente_meses'] = np.nan\n",
        "\n",
        "# Crear rangos de antigüedad\n",
        "bins = [0, 6, 12, 24, 60, 120, float('inf')]\n",
        "labels = ['<6m', '6-12m', '1-2a', '2-5a', '5-10a', '10+a']\n",
        "df['antiguedad_rango'] = pd.cut(df['antiguedad_cliente_meses'], bins=bins, labels=labels)\n",
        "\n",
        "# Derivados temporales (si hay fecha de encuesta)\n",
        "if fecha_encuesta_col:\n",
        "    df['mes_encuesta'] = df[fecha_encuesta_col].dt.month\n",
        "    df['dia_semana_encuesta'] = df[fecha_encuesta_col].dt.weekday\n",
        "    df['hora_encuesta'] = df[fecha_encuesta_col].dt.hour\n",
        "\n",
        "print(\"Shape inicial:\", df.shape)\n",
        "print(\"Primeras columnas:\", df.columns[:50].tolist())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rZv2Ex2gzJ3R",
      "metadata": {
        "id": "rZv2Ex2gzJ3R"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Exploración visual\n",
        "*   ### CSAT promedio por rango de antigüedad:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60614e1",
      "metadata": {
        "id": "c60614e1"
      },
      "outputs": [],
      "source": [
        "# === Bloque misceláneo ===\n",
        "# Asegurarse de que CSAT sea numérico\n",
        "df['CSAT General'] = pd.to_numeric(df['CSAT General'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b5eb0f",
      "metadata": {
        "id": "f6b5eb0f"
      },
      "outputs": [],
      "source": [
        "# === Visualización (EDA/Resultados) ===\n",
        "# Gráficos para entender distribución, relaciones y resultados del modelo.\n",
        "#Visualizar la distribucion del CSAT por diferentes variable\n",
        "\n",
        "# Promedio de CSAT por rango\n",
        "csat_por_antig = df.groupby('antiguedad_rango')['CSAT General'].mean()\n",
        "csat_por_producto = df.groupby('grupo_producto')['CSAT General'].mean()\n",
        "csat_por_producto = df.groupby('motivo_contacto')['CSAT General'].mean()\n",
        "\n",
        "# Gráfico\n",
        "csat_por_antig.plot(kind='bar', title='Promedio de CSAT por antigüedad del cliente', color='skyblue')\n",
        "plt.ylabel('CSAT promedio')\n",
        "plt.xlabel('Rango de antigüedad')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rrA85EUHzjPZ",
      "metadata": {
        "id": "rrA85EUHzjPZ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "*   ## Analisis de nulos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b1422c",
      "metadata": {
        "id": "57b1422c"
      },
      "outputs": [],
      "source": [
        "# === Explicabilidad con SHAP ===\n",
        "# Importancia global (bar/beeswarm) y dependence plots para variables top.\n",
        "nulos = df.isnull().sum().to_frame(name='Cantidad de nulos')\n",
        "nulos['% de nulos'] = (nulos['Cantidad de nulos'] / len(df) * 100).round(2)\n",
        "nulos = nulos[nulos['Cantidad de nulos'] > 0].sort_values(by='% de nulos', ascending=False)\n",
        "display(nulos.head(100))\n",
        "df.isnull().sum()*100/df.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpieza / Normalización: manejo de nulos, tipos, merges y depuración\n",
        "##### Objetivo: dejar los datos consistentes antes del EDA/modelado."
      ],
      "metadata": {
        "id": "AHhfNiyGHVlG"
      },
      "id": "AHhfNiyGHVlG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f0d6e0e",
      "metadata": {
        "id": "5f0d6e0e"
      },
      "outputs": [],
      "source": [
        "# CSAT General -> binaria: 1 si CSAT >= 4, 0 en caso contrario\n",
        "assert 'CSAT General' in df.columns, \"No se encontró 'CSAT General' en el dataset.\"\n",
        "df['CSAT_bin'] = (df['CSAT General'] >= 4).astype(int)\n",
        "\n",
        "#Eliminamos variables de fuga relacionadas a resolución (si están)\n",
        "fuga_patterns = ['resoluc', 'Respuesta Resolucion','¿Resolvimos tu consulta?']\n",
        "cols_fuga = [c for c in df.columns if any(pat.lower() in c.lower() for pat in fuga_patterns)]\n",
        "df = df.drop(columns=cols_fuga, errors='ignore')\n",
        "\n",
        "print(\"Columnas de fuga eliminadas:\", cols_fuga)\n",
        "print(\"Distribución CSAT_bin:\\n\", df['CSAT_bin'].value_counts(normalize=True).round(4)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5e4e78",
      "metadata": {
        "id": "3b5e4e78"
      },
      "outputs": [],
      "source": [
        "# Elegir columna origen\n",
        "posibles_motivo = ['motivo_contacto']\n",
        "motivo_col = next((c for c in posibles_motivo if c in df.columns), None)\n",
        "\n",
        "#Agrupamos los motivos de contacto\n",
        "def agrupar_motivos_texto(valor):\n",
        "    if pd.isna(valor):\n",
        "        return 'Otro'\n",
        "    v = str(valor).lower()\n",
        "    # Tarjeta\n",
        "    if any(k in v for k in ['tarjeta', 'plástico', 'pin', 'visa', 'mastercard', 'extravio', 'bloqueo']):\n",
        "        return 'Tarjeta'\n",
        "    # Préstamos / créditos\n",
        "    if any(k in v for k in ['préstam', 'prestam', 'cuota', 'sobregiro', 'credito', 'crédito']):\n",
        "        return 'Préstamos'\n",
        "    # Transferencias / pagos / QR / billeteras / ATM\n",
        "    if any(k in v for k in ['transferen', 'pago', 'qr', 'ted', 'atm', 'alias', 'debito', 'débito', 'swift', 'cnb']):\n",
        "        return 'Transferencias/ Pagos'\n",
        "    # Cuenta / App / Acceso / Onboarding\n",
        "    if any(k in v for k in ['cuenta', 'onboarding', 'contraseña', 'password', 'app', 'login', 'web', 'canal']):\n",
        "        return 'Cuenta/App'\n",
        "    # Ofertas / Comercial / Promos\n",
        "    if any(k in v for k in ['oferta', 'promoc', 'comercial', 'venta', 'promo','loyal']):\n",
        "        return 'Ofertas/Comercial'\n",
        "    # Fraude / seguridad\n",
        "    if any(k in v for k in ['fraude', 'estafa', 'seguridad', 'vulneración', 'robo']):\n",
        "        return 'Fraude/Seguridad'\n",
        "    # Inversiones / CDA / fondos / casa de bolsa\n",
        "    if any(k in v for k in ['cda', 'fondo', 'invers', 'bolsa']):\n",
        "        return 'Inversiones'\n",
        "    # Servicios / facturas\n",
        "    if any(k in v for k in ['servicio', 'factura', 'facturador']):\n",
        "        return 'Servicios'\n",
        "    # Generales / Operativos\n",
        "    if any(k in v for k in ['consulta', 'requisito', 'condicion', 'soporte', 'ayuda', 'informacion', 'información']):\n",
        "        return 'Operativo/Soporte'\n",
        "    return 'Otro'\n",
        "\n",
        "if motivo_col:\n",
        "    if df[motivo_col].dtype == object:\n",
        "        df['motivo_contacto_categoria'] = df[motivo_col].apply(agrupar_motivos_texto)\n",
        "    else:\n",
        "        # Si no tenés el texto original, vas a agrupar a 'Otro'\n",
        "        df['motivo_contacto_categoria'] = 'Otro'\n",
        "else:\n",
        "    df['motivo_contacto_categoria'] = 'Otro'\n",
        "\n",
        "df['motivo_contacto_categoria'] = df['motivo_contacto_categoria'].fillna('Otro')\n",
        "\n",
        "print(\"Top categorías motivo_contacto_categoria:\")\n",
        "print(df['motivo_contacto_categoria'].value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903575e0",
      "metadata": {
        "id": "903575e0"
      },
      "outputs": [],
      "source": [
        "# === Bloque misceláneo ===\n",
        "for name in ['X','y','X_train','X_test','y_train','y_test','X_train_scaled','X_test_scaled']:\n",
        "    if name in globals():\n",
        "        del globals()[name]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ingeniería de variables: codificaciones y derivados (freq/target mean, one-hot, etc.)\n",
        "#### Nota: evita data leakage (calcula sobre training o usa pipelines/column_transformers)."
      ],
      "metadata": {
        "id": "AZ2JalF_Hg-F"
      },
      "id": "AZ2JalF_Hg-F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92095a4f",
      "metadata": {
        "id": "92095a4f"
      },
      "outputs": [],
      "source": [
        "# 1) Lista blanca de columnas (ajustá nombres según existan en tu df)\n",
        "whitelist = [\n",
        "    'antiguedad_cliente_meses','rango_etario','genero','ya_es_cliente','Recontacto_sf','Producto_sf','Subproduct_sf','motivo_contacto_categoria'\n",
        "]\n",
        "\n",
        "# 2) Construir df_modelo solo con whitelist + target\n",
        "cols_existentes = [c for c in whitelist if c in df.columns]\n",
        "df_modelo = df[cols_existentes + ['CSAT_bin']].copy()\n",
        "\n",
        "print(\"✅ Columnas usadas:\", df_modelo.columns.tolist())\n",
        "\n",
        "# 3) (doble seguro) No permitir CSAT* ni Resolucion* en features\n",
        "ban_patterns = ['csat', 'satisf', 'resoluc']\n",
        "cols_ban = [c for c in df_modelo.columns if any(p in c.lower() for p in ban_patterns) and c != 'CSAT_bin']\n",
        "if cols_ban:\n",
        "    df_modelo = df_modelo.drop(columns=cols_ban)\n",
        "    print(\"🚫 Quitadas por seguridad:\", cols_ban)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b68fb2f",
      "metadata": {
        "id": "5b68fb2f"
      },
      "outputs": [],
      "source": [
        "# 4) Separar X/y\n",
        "X = df_modelo.drop(columns=['CSAT_bin']).copy()\n",
        "y = df_modelo['CSAT_bin'].astype(int)\n",
        "\n",
        "# 5) Codificar solo las categóricas que queden en X\n",
        "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
        "label_encoders = {}\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "print(\"🔤 Categóricas codificadas:\", cat_cols)\n",
        "\n",
        "# 6) Imputación (por si queda algún NaN)\n",
        "imp = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imp.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# 7) Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"✅ X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
        "print(\"✅ X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
        "\n",
        "print(\"📊 y_train %:\\n\", (y_train.value_counts(normalize=True)*100).round(2))\n",
        "\n",
        "\n",
        "resumen = pd.DataFrame({\n",
        "    'Variable': X.columns,\n",
        "    'dtype': X.dtypes.values,\n",
        "    '% nulos': (X.isnull().mean()*100).round(2),\n",
        "    'n_únicos': [X[c].nunique() for c in X.columns]\n",
        "}).sort_values('% nulos', ascending=False)\n",
        "resumen.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explicabilidad con SHAP"
      ],
      "metadata": {
        "id": "H69uTWWFH5v_"
      },
      "id": "H69uTWWFH5v_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876eefc3",
      "metadata": {
        "id": "876eefc3"
      },
      "outputs": [],
      "source": [
        "# Importancia global (bar/beeswarm) y dependence plots para variables top.\n",
        "print(\"✅ Columnas usadas:\", df_modelo.columns.tolist())\n",
        "\n",
        "# Análisis de valores nulos\n",
        "nulos = df_modelo.isnull().sum().to_frame(name='Cantidad de nulos')\n",
        "nulos['% de nulos'] = (nulos['Cantidad de nulos'] / len(df) * 100).round(2)\n",
        "nulos = nulos[nulos['Cantidad de nulos'] > 0].sort_values(by='% de nulos', ascending=False)\n",
        "nulos.head(20)\n",
        "\n",
        "print(\"🔧 Features usadas por el modelo:\", X.columns.tolist())\n",
        "print(\"Shape X:\", X.shape, \" | Shape y:\", y.shape)\n",
        "print(\"\\nTipos de datos:\")\n",
        "print(X.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "205fdbcb",
      "metadata": {
        "id": "205fdbcb"
      },
      "outputs": [],
      "source": [
        "# Eliminar columnas con 100% NaNs\n",
        "columnas_100_nan = X_train.columns[X_train.isnull().all()]\n",
        "print(\"🧼 Columnas eliminadas por tener 100% NaNs:\", columnas_100_nan.tolist())\n",
        "\n",
        "X_train = X_train.drop(columns=columnas_100_nan)\n",
        "X_test = X_test.drop(columns=columnas_100_nan, errors='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Entrenamiento del modelo**"
      ],
      "metadata": {
        "id": "acXoi6HWIA3M"
      },
      "id": "acXoi6HWIA3M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Regresión logística (simple y fácil de interpretar)"
      ],
      "metadata": {
        "id": "FGfSnIbtIibO"
      },
      "id": "FGfSnIbtIibO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0deede25",
      "metadata": {
        "id": "0deede25"
      },
      "outputs": [],
      "source": [
        "# Define el estimador (e.g., XGBoost/RandomForest/Logística) y entrena con los features preparados.\n",
        "#Paso: Entrenamiento y evaluación de modelo base\n",
        "#Usaremos:\n",
        "#Regresión logística (simple y fácil de interpretar)\n",
        "#Métricas: accuracy, precision, recall, f1_score, roc_auc\n",
        "#Matriz de confusión\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "modelo_log = LogisticRegression(max_iter=8000)\n",
        "modelo_log.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_log = modelo_log.predict(X_test_scaled)\n",
        "y_proba_log = modelo_log.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"📌 Clasification Report — Regresión Logística (sin resolución):\")\n",
        "print(classification_report(y_test, y_pred_log))\n",
        "print(\"📈 ROC-AUC:\", roc_auc_score(y_test, y_proba_log))\n",
        "print(\"--\")\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_test, y_pred_log)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['No Satisfactorio', 'Satisfactorio'], yticklabels=['No Satisfactorio', 'Satisfactorio'])\n",
        "plt.title(\"Matriz de Confusión — Logística (sin resolución)\")\n",
        "plt.xlabel(\"Predicción\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ec1f0f",
      "metadata": {
        "id": "d0ec1f0f"
      },
      "outputs": [],
      "source": [
        "# === Visualización (EDA/Resultados) ===\n",
        "# Gráficos para entender distribución, relaciones y resultados del modelo.\n",
        "# Gráfico para visualizar la relación entre Recontacto y CSAT\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='CSAT General', hue='Recontacto_sf')\n",
        "plt.title('Distribución de CSAT General según si hubo Recontacto')\n",
        "plt.xlabel('Puntuación CSAT General')\n",
        "plt.ylabel('Cantidad de Clientes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209a7cf8",
      "metadata": {
        "id": "209a7cf8"
      },
      "outputs": [],
      "source": [
        "# === Visualización (EDA/Resultados) ===\n",
        "# Gráficos para entender distribución, relaciones y resultados del modelo.\n",
        "#Extraer el coeficiente\n",
        "coeficientes = modelo_log.coef_[0]\n",
        "variables = X_train.columns\n",
        "\n",
        "#Crear dataframe\n",
        "df_coef = pd.DataFrame({\n",
        "    'Variable': variables,\n",
        "    'Coeficiente': coeficientes\n",
        "}).sort_values(by='Coeficiente', ascending=False)\n",
        "\n",
        "# 🔝 Mostrar top 15 positivos y negativos\n",
        "top_pos = df_coef.head(15)\n",
        "top_neg = df_coef.tail(15)\n",
        "\n",
        "print(top_pos)\n",
        "\n",
        "# 📊 Gráfico de barras\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(top_pos['Variable'], top_pos['Coeficiente'], color='green')\n",
        "plt.title(\"🔼 Variables que aumentan la probabilidad de satisfacción (CSAT=1)\")\n",
        "plt.xlabel(\"Coeficiente\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(top_pos)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(top_neg['Variable'], top_neg['Coeficiente'], color='red')\n",
        "plt.title(\"🔽 Variables que disminuyen la probabilidad de satisfacción (CSAT=1)\")\n",
        "plt.xlabel(\"Coeficiente\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####RandomForest"
      ],
      "metadata": {
        "id": "vD6gx8G5IbLC"
      },
      "id": "vD6gx8G5IbLC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65077bbc",
      "metadata": {
        "id": "65077bbc"
      },
      "outputs": [],
      "source": [
        "# === Entrenamiento del modelo ===\n",
        "# Define el estimador (e.g., XGBoost/RandomForest/Logística) y entrena con los features preparados.\n",
        "modelo_rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "modelo_rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = modelo_rf.predict(X_test)\n",
        "y_proba_rf = modelo_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"📌 Clasification Report — Random Forest (sin resolución):\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"📈 ROC-AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
        "print(\"----\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['No Satisfactorio', 'Satisfactortio'], yticklabels=['No Satisfactorio', 'Satisfactorio'])\n",
        "plt.title(\"Matriz de Confusión — RF (sin resolución)\")\n",
        "plt.xlabel(\"Predicción\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gi5RMurvGI2r",
      "metadata": {
        "id": "gi5RMurvGI2r"
      },
      "source": [
        "**XGBoost con pesos + early stopping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c23226",
      "metadata": {
        "id": "50c23226"
      },
      "outputs": [],
      "source": [
        "# ===== XGBoost con early stopping (API nativa) + pesos clase 0 + ajuste de umbral =====\n",
        "print(\"XGBoost version:\", xgb.__version__)\n",
        "\n",
        "# 1) Split interno para validación\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.10, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "# 2) Pesos para favorecer la clase 0 (insatisfechos)\n",
        "ratio = (y_tr == 1).sum() / max(1, (y_tr == 0).sum())\n",
        "sample_w = np.where(y_tr == 0, ratio, 1.0)\n",
        "\n",
        "# 3) DMatrix (requerido por API nativa)\n",
        "dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=sample_w)\n",
        "dvalid = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest  = xgb.DMatrix(X_test)\n",
        "\n",
        "# 4) Parámetros equivalentes\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'max_depth': 5,\n",
        "    'eta': 0.05,                # = learning_rate\n",
        "    'subsample': 0.9,\n",
        "    'colsample_bytree': 0.9,\n",
        "    'min_child_weight': 1,\n",
        "    'lambda': 1.0,              # = reg_lambda\n",
        "    'alpha': 0.0,               # = reg_alpha\n",
        "    'tree_method': 'hist',\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# 5) Entrenamiento con early stopping estable\n",
        "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "booster = xgb.train(\n",
        "    params=params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=2000,\n",
        "    evals=watchlist,\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=False\n",
        ")\n",
        "print(f\"Mejor iteración: {booster.best_iteration} | Mejor logloss(valid): {booster.best_score:.4f}\")\n",
        "\n",
        "# 6) Predicción en TEST respetando mejor iteración\n",
        "try:\n",
        "    y_proba = booster.predict(dtest, iteration_range=(0, booster.best_iteration + 1))\n",
        "except TypeError:\n",
        "    # Fallback por compatibilidad\n",
        "    y_proba = booster.predict(dtest)\n",
        "\n",
        "# --- Report con umbral 0.50 ---\n",
        "y_pred = (y_proba >= 0.50).astype(int)\n",
        "print(\"📌 Report (umbral 0.50):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=['No Sat', 'Sat'], yticklabels=['No Sat', 'Sat'])\n",
        "plt.title(\"Matriz — XGB (umbral 0.50)\")\n",
        "plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.show()\n",
        "\n",
        "# 7) Búsqueda de umbral óptimo para maximizar F1 de la clase 0\n",
        "def umbral_optimo_clase0(y_true, proba_pos):\n",
        "    best = {'thr':0.5, 'f1_0':-1, 'rec0':0, 'prec0':0}\n",
        "    for thr in np.linspace(0.1, 0.9, 81):\n",
        "        y_hat = (proba_pos >= thr).astype(int)        # proba de clase 1\n",
        "        f1_0  = f1_score(y_true, y_hat, pos_label=0)\n",
        "        rec0  = recall_score(y_true, y_hat, pos_label=0)\n",
        "        prec0 = precision_score(y_true, y_hat, pos_label=0)\n",
        "        if f1_0 > best['f1_0']:\n",
        "            best = {'thr':thr, 'f1_0':f1_0, 'rec0':rec0, 'prec0':prec0}\n",
        "    return best\n",
        "\n",
        "best = umbral_optimo_clase0(y_test, y_proba)\n",
        "print(\"🔎 Umbral óptimo para clase 0:\", best)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Limpieza / Normalización: manejo de nulos, tipos, merges y depuración"
      ],
      "metadata": {
        "id": "WQc6eXC_Is60"
      },
      "id": "WQc6eXC_Is60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b41bec",
      "metadata": {
        "id": "10b41bec"
      },
      "outputs": [],
      "source": [
        "# Objetivo: dejar los datos consistentes antes del EDA/modelado.\n",
        "# --- Report con umbral óptimo para clase 0 ---\n",
        "y_pred_opt = (y_proba >= best['thr']).astype(int)\n",
        "print(\"📌 Report (umbral óptimo clase 0):\")\n",
        "print(classification_report(y_test, y_pred_opt))\n",
        "\n",
        "cm_opt = confusion_matrix(y_test, y_pred_opt)\n",
        "sns.heatmap(cm_opt, annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=['No Sat', 'Sat'], yticklabels=['No Sat', 'Sat'])\n",
        "plt.title(\"Matriz — XGB (umbral óptimo clase 0)\")\n",
        "plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8607f315",
      "metadata": {
        "id": "8607f315"
      },
      "outputs": [],
      "source": [
        "# elegir umbral por objetivo de negocio o costo\n",
        "\n",
        "def umbral_por_objetivo(y_true, proba_pos, min_recall0=0.70):\n",
        "    mejor = {'thr':0.5, 'rec0':0, 'f1_0':-1}\n",
        "    for thr in np.linspace(0.1, 0.9, 81):\n",
        "        y_hat = (proba_pos >= thr).astype(int)\n",
        "        rec0  = recall_score(y_true, y_hat, pos_label=0)\n",
        "        f1_0v = f1_score(y_true, y_hat, pos_label=0)\n",
        "        if rec0 >= min_recall0 and f1_0v > mejor['f1_0']:\n",
        "            mejor = {'thr':thr, 'rec0':rec0, 'f1_0':f1_0v}\n",
        "    return mejor\n",
        "\n",
        "def umbral_por_coste(y_true, proba_pos, cost_fn=5.0, cost_fp=1.0):\n",
        "    mejor = {'thr':0.5, 'costo':float('inf')}\n",
        "    for thr in np.linspace(0.1, 0.9, 81):\n",
        "        y_hat = (proba_pos >= thr).astype(int)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
        "        costo = cost_fn*fn + cost_fp*fp\n",
        "        if costo < mejor['costo']:\n",
        "            mejor = {'thr':thr, 'costo':costo}\n",
        "    return mejor\n",
        "\n",
        "# Usa y_proba de la celda XGB nativa\n",
        "tgt = umbral_por_objetivo(y_test, y_proba, min_recall0=0.70)\n",
        "print(\"🎯 Umbral con recall0 ≥ 0.70:\", tgt)\n",
        "\n",
        "y_hat_obj = (y_proba >= tgt['thr']).astype(int)\n",
        "print(classification_report(y_test, y_hat_obj))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079b855d",
      "metadata": {
        "id": "079b855d"
      },
      "outputs": [],
      "source": [
        "# XGBoost RandomizedSearchCV pro-clase 0 (opcional)\n",
        "\n",
        "# Pesos para favorecer clase 0 en el fit\n",
        "w_ratio = (y_train==1).sum() / (y_train==0).sum()\n",
        "sw = np.where(y_train==0, w_ratio, 1.0)\n",
        "\n",
        "# Scorers que priorizan la clase 0\n",
        "f1_0     = make_scorer(f1_score,    pos_label=0)\n",
        "recall_0 = make_scorer(recall_score,pos_label=0)\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "xgb_base = XGBClassifier(\n",
        "    eval_metric='logloss',\n",
        "    tree_method='hist',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "param_distributions = {\n",
        "    'n_estimators':     [300, 600, 900, 1200],\n",
        "    'learning_rate':    [0.03, 0.05, 0.08, 0.1],\n",
        "    'max_depth':        [4, 5, 6, 8],\n",
        "    'min_child_weight': [1, 2, 5],\n",
        "    'subsample':        [0.7, 0.85, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.85, 1.0],\n",
        "    'gamma':            [0, 0.3, 1.0],\n",
        "    'reg_lambda':       [1.0, 3.0, 6.0],\n",
        "    'reg_alpha':        [0.0, 0.1, 0.5]\n",
        "}\n",
        "\n",
        "rand = RandomizedSearchCV(\n",
        "    estimator=xgb_base,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=30,\n",
        "    scoring={'f1_0': f1_0, 'recall_0': recall_0, 'bacc': 'balanced_accuracy'},\n",
        "    refit='f1_0',\n",
        "    cv=cv,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rand.fit(X_train, y_train, sample_weight=sw)\n",
        "print(\"🔧 Mejores hiperparámetros:\", rand.best_params_)\n",
        "best_xgb = rand.best_estimator_\n",
        "\n",
        "# Evaluación en test con umbral 0.50\n",
        "y_proba_rs = best_xgb.predict_proba(X_test)[:,1]\n",
        "y_pred_rs  = (y_proba_rs >= 0.50).astype(int)\n",
        "print(\"📌 Report Randomized (0.50):\")\n",
        "print(classification_report(y_test, y_pred_rs))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_rs))\n",
        "\n",
        "# (opcional) aplicar el mismo criterio de umbral que en la celda A:\n",
        "tgt_rs = umbral_por_objetivo(y_test, y_proba_rs, min_recall0=0.70)\n",
        "print(\"🎯 Umbral RS con recall0 ≥ 0.70:\", tgt_rs)\n",
        "y_hat_rs = (y_proba_rs >= tgt_rs['thr']).astype(int)\n",
        "print(classification_report(y_test, y_hat_rs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b48f0f9",
      "metadata": {
        "id": "7b48f0f9"
      },
      "outputs": [],
      "source": [
        "# === Limpieza / Normalización: manejo de nulos, tipos, merges y depuración ===\n",
        "# Objetivo: dejar los datos consistentes antes del EDA/modelado.\n",
        "# Elegí en cuáles columnas aplicar encoding adicional (de las que TENÉS en X)\n",
        "cand_cols = [c for c in ['Producto_sf','Subproduct_sf','grupo_producto',\n",
        "                         'motivo_contacto_categoria','rango_etario','genero',\n",
        "                         'ya_es_cliente']\n",
        "             if c in X_train.columns]\n",
        "\n",
        "def add_freq_and_target_mean(Xtr, Xte, ytr, cols, positive_class=0):\n",
        "    Xtr = Xtr.copy(); Xte = Xte.copy()\n",
        "    for col in cols:\n",
        "        # Frequency encoding (proporción de cada categoría en train)\n",
        "        freq = Xtr[col].value_counts(normalize=True)\n",
        "        Xtr[f'{col}_freq'] = Xtr[col].map(freq)\n",
        "        Xte[f'{col}_freq'] = Xte[col].map(freq).fillna(0)\n",
        "\n",
        "        # Target-mean encoding para clase 0 (prob de ser clase 0 por categoría)\n",
        "        # Nota: calculado SOLO en train para evitar fuga\n",
        "        tmp = pd.DataFrame({col: Xtr[col], 'y': ytr})\n",
        "        # prob de clase 0:\n",
        "        mean0 = tmp.groupby(col)['y'].apply(lambda v: (v==positive_class).mean())\n",
        "        global_mean0 = (ytr==positive_class).mean()\n",
        "        Xtr[f'{col}_mean0'] = Xtr[col].map(mean0)\n",
        "        Xte[f'{col}_mean0'] = Xte[col].map(mean0).fillna(global_mean0)\n",
        "\n",
        "    return Xtr, Xte\n",
        "\n",
        "X_train_ext, X_test_ext = add_freq_and_target_mean(X_train, X_test, y_train, cand_cols, positive_class=0)\n",
        "print(\"Columns added:\",\n",
        "      [c for c in X_train_ext.columns if c.endswith('_freq') or c.endswith('_mean0')])\n",
        "print(\"Shapes -> X_train_ext:\", X_train_ext.shape, \"| X_test_ext:\", X_test_ext.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c9f2c1",
      "metadata": {
        "id": "87c9f2c1"
      },
      "outputs": [],
      "source": [
        "# === Bloque misceláneo ===\n",
        "print(\"X_train cols:\", X_train.columns.tolist())\n",
        "\n",
        "candidatas = ['Producto_sf','Subproduct_sf','grupo_producto',\n",
        "              'motivo_contacto_categoria','rango_etario','genero','ya_es_cliente',\n",
        "              'ciudad','Canal','canal_ticket']\n",
        "faltan = [c for c in candidatas if c not in X_train.columns]\n",
        "print(\"❗ No están en X_train:\", faltan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bdcb1f5",
      "metadata": {
        "id": "7bdcb1f5"
      },
      "outputs": [],
      "source": [
        "# === Limpieza / Normalización: manejo de nulos, tipos, merges y depuración ===\n",
        "# Objetivo: dejar los datos consistentes antes del EDA/modelado.\n",
        "\n",
        "# Split interno para early stopping\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train_ext, y_train, test_size=0.10, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "# Pesos a clase 0 (minoritaria)\n",
        "ratio = (y_tr==1).sum() / max(1, (y_tr==0).sum())\n",
        "sw = np.where(y_tr==0, ratio, 1.0)\n",
        "\n",
        "# DMatrix\n",
        "dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=sw)\n",
        "dvalid = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest  = xgb.DMatrix(X_test_ext)\n",
        "\n",
        "# Parámetros (conservadores, buenos para recall_0)\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'max_depth': 5,\n",
        "    'eta': 0.05,\n",
        "    'subsample': 0.9,\n",
        "    'colsample_bytree': 0.9,\n",
        "    'min_child_weight': 2,\n",
        "    'lambda': 1.0,\n",
        "    'alpha': 0.5,\n",
        "    'tree_method': 'hist',\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "booster = xgb.train(\n",
        "    params=params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=2000,\n",
        "    evals=[(dtrain,'train'), (dvalid,'valid')],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=False\n",
        ")\n",
        "\n",
        "print(f\"Mejor iteración: {booster.best_iteration} | Mejor logloss(valid): {booster.best_score:.4f}\")\n",
        "\n",
        "# Predicciones test\n",
        "y_proba_ext = booster.predict(dtest, iteration_range=(0, booster.best_iteration + 1))\n",
        "y_pred05    = (y_proba_ext >= 0.50).astype(int)\n",
        "\n",
        "print(\"📌 Report (umbral 0.50) con features extra:\")\n",
        "print(classification_report(y_test, y_pred05))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_ext))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred05)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=['No Sat','Sat'], yticklabels=['No Sat','Sat'])\n",
        "plt.title(\"Matriz — XGB + (freq + mean0) — umbral 0.50\")\n",
        "plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67404a2f",
      "metadata": {
        "id": "67404a2f"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def umbral_por_objetivo(y_true, proba_pos, min_recall0=0.70):\n",
        "    mejor = {'thr':0.5, 'rec0':0, 'f1_0':-1}\n",
        "    for thr in np.linspace(0.1, 0.9, 81):\n",
        "        y_hat = (proba_pos >= thr).astype(int)\n",
        "        rec0  = recall_score(y_true, y_hat, pos_label=0)\n",
        "        f1_0v = f1_score(y_true, y_hat, pos_label=0)\n",
        "        if rec0 >= min_recall0 and f1_0v > mejor['f1_0']:\n",
        "            mejor = {'thr':thr, 'rec0':rec0, 'f1_0':f1_0v}\n",
        "    return mejor\n",
        "\n",
        "best_obj = umbral_por_objetivo(y_test, y_proba_ext, min_recall0=0.70)\n",
        "print(\"🎯 Umbral con recall0 ≥ 0.70:\", best_obj)\n",
        "\n",
        "y_hat_obj = (y_proba_ext >= best_obj['thr']).astype(int)\n",
        "print(\"📌 Report (umbral objetivo):\")\n",
        "print(classification_report(y_test, y_hat_obj))\n",
        "\n",
        "cm2 = confusion_matrix(y_test, y_hat_obj)\n",
        "sns.heatmap(cm2, annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=['No Sat','Sat'], yticklabels=['No Sat','Sat'])\n",
        "plt.title(\"Matriz — XGB + (freq + mean0) — umbral objetivo\")\n",
        "plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e91fb6",
      "metadata": {
        "id": "b7e91fb6"
      },
      "outputs": [],
      "source": [
        "# === Bloque misceláneo ===\n",
        "#Para evitar que variables viejas se mezclen, podés limpiar algunas antes de empezar la parte nueva:\n",
        "\n",
        "for name in ['X_train_ext','X_test_ext','y_proba','y_pred']:\n",
        "    if name in globals(): del globals()[name]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3125971",
      "metadata": {
        "id": "a3125971"
      },
      "outputs": [],
      "source": [
        "# === Limpieza / Normalización: manejo de nulos, tipos, merges y depuración ===\n",
        "# Objetivo: dejar los datos consistentes antes del EDA/modelado.\n",
        "# Celda NUEVA: features extra\n",
        "cand_cols = [c for c in [\n",
        "    'Producto_sf','Subproduct_sf','grupo_producto','motivo_contacto_categoria',\n",
        "    'rango_etario','genero','ya_es_cliente','ciudad','Canal','canal_ticket'\n",
        "] if c in X_train.columns]\n",
        "\n",
        "def add_freq_and_target_mean(Xtr, Xte, ytr, cols, positive_class=0):\n",
        "    Xtr = Xtr.copy(); Xte = Xte.copy()\n",
        "    base_p0 = (ytr == positive_class).mean()  # prob global de clase 0\n",
        "\n",
        "    for col in cols:\n",
        "        # Frequency encoding\n",
        "        freq = Xtr[col].value_counts(normalize=True)\n",
        "        Xtr[f'{col}_freq'] = Xtr[col].map(freq)\n",
        "        Xte[f'{col}_freq'] = Xte[col].map(freq).fillna(0)\n",
        "\n",
        "        # Target-mean encoding (prob de ser clase 0)\n",
        "        tmp = pd.DataFrame({col: Xtr[col], 'y': ytr.values})\n",
        "        # y es 0/1 → prob0 = 1 - mean(y)\n",
        "        mean1 = tmp.groupby(col)['y'].mean()        # P(y=1 | col)\n",
        "        prob0 = 1.0 - mean1                         # P(y=0 | col)\n",
        "        Xtr[f'{col}_mean0'] = Xtr[col].map(prob0)\n",
        "        Xte[f'{col}_mean0'] = Xte[col].map(prob0).fillna(base_p0)\n",
        "\n",
        "    return Xtr, Xte\n",
        "\n",
        "\n",
        "X_train_ext, X_test_ext = add_freq_and_target_mean(X_train, X_test, y_train, cand_cols, positive_class=0)\n",
        "\n",
        "print(\"➕ Nuevas columnas:\", [c for c in X_train_ext.columns if c.endswith('_freq') or c.endswith('_mean0')])\n",
        "print(\"📐 Shapes:\", X_train_ext.shape, X_test_ext.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b433c47",
      "metadata": {
        "id": "3b433c47"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "# Celda NUEVA: XGBoost nativo + early stopping con X_train_ext/X_test_ext\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, f1_score, recall_score\n",
        "import numpy as np, seaborn as sns, matplotlib.pyplot as plt\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train_ext, y_train, test_size=0.10, random_state=42, stratify=y_train\n",
        ")\n",
        "ratio = (y_tr==1).sum() / max(1,(y_tr==0).sum())\n",
        "sw = np.where(y_tr==0, ratio, 1.0)\n",
        "\n",
        "dtrain = xgb.DMatrix(X_tr,  label=y_tr, weight=sw)\n",
        "dvalid = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest  = xgb.DMatrix(X_test_ext)\n",
        "\n",
        "params = {\n",
        "    'objective':'binary:logistic','eval_metric':'logloss','max_depth':5,'eta':0.05,\n",
        "    'subsample':0.9,'colsample_bytree':0.9,'min_child_weight':2,'lambda':1.0,'alpha':0.5,\n",
        "    'tree_method':'hist','seed':42\n",
        "}\n",
        "booster = xgb.train(params, dtrain, num_boost_round=2000,\n",
        "                    evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "                    early_stopping_rounds=50, verbose_eval=False)\n",
        "\n",
        "print(f\"Mejor iteración: {booster.best_iteration} | logloss(valid): {booster.best_score:.4f}\")\n",
        "\n",
        "y_proba = booster.predict(dtest, iteration_range=(0, booster.best_iteration+1))\n",
        "y_pred  = (y_proba >= 0.50).astype(int)\n",
        "print(\"📌 Report (0.50):\"); print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=['No Sat','Sat'], yticklabels=['No Sat','Sat'])\n",
        "plt.title(\"Matriz — XGB + encodings (umbral 0.50)\"); plt.show()\n",
        "\n",
        "# Umbral por objetivo de negocio (ej. recall0 ≥ 0.70)\n",
        "def umbral_por_objetivo(y_true, proba_pos, min_recall0=0.70):\n",
        "    best = {'thr':0.5, 'rec0':0, 'f1_0':-1}\n",
        "    for thr in np.linspace(0.1,0.9,81):\n",
        "        y_hat = (proba_pos >= thr).astype(int)\n",
        "        rec0  = recall_score(y_true, y_hat, pos_label=0)\n",
        "        f1_0v = f1_score(y_true, y_hat, pos_label=0)\n",
        "        if rec0 >= min_recall0 and f1_0v > best['f1_0']:\n",
        "            best = {'thr':thr, 'rec0':rec0, 'f1_0':f1_0v}\n",
        "    return best\n",
        "\n",
        "best_obj = umbral_por_objetivo(y_test, y_proba, min_recall0=0.70)\n",
        "print(\"🎯 Umbral con recall0 ≥ 0.70:\", best_obj)\n",
        "y_hat_obj = (y_proba >= best_obj['thr']).astype(int)\n",
        "print(\"📌 Report (umbral objetivo):\"); print(classification_report(y_test, y_hat_obj))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1040f7e0",
      "metadata": {
        "id": "1040f7e0"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "umbral_final = 0.58  # ajustalo con el valor que te salió (p.ej. 0.58)\n",
        "y_pred_final = (y_proba >= umbral_final).astype(int)\n",
        "\n",
        "print(f\"👉 Umbral fijado: {umbral_final:.2f}\")\n",
        "print(classification_report(y_test, y_pred_final))\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_final).ravel()\n",
        "print(\"Confusión:\", {'tn':int(tn),'fp':int(fp),'fn':int(fn),'tp':int(tp)})\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "resultados = {\n",
        "    'umbral': umbral_final,\n",
        "    'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp),\n",
        "    'auc': float(roc_auc_score(y_test, y_proba))\n",
        "}\n",
        "resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982ae95d",
      "metadata": {
        "id": "982ae95d"
      },
      "outputs": [],
      "source": [
        "# === Bloque misceláneo ===\n",
        "# Importancias (ganancia) del booster nativo\n",
        "imp = booster.get_score(importance_type='gain')\n",
        "top10 = sorted(imp.items(), key=lambda x: -x[1])[:10]\n",
        "print(\"🔝 Top 10 features (gain):\")\n",
        "for k,v in top10:\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2c8947b",
      "metadata": {
        "id": "d2c8947b"
      },
      "outputs": [],
      "source": [
        "# === Limpieza / Normalización: manejo de nulos, tipos, merges y depuración ===\n",
        "# Objetivo: dejar los datos consistentes antes del EDA/modelado.\n",
        "# 🔒 Experimento A/B: quitar Recontacto_sf y reentrenar rápido sobre las features extendidas\n",
        "cols_sin_recontacto = [c for c in X_train_ext.columns if c != 'Recontacto_sf']\n",
        "dtrain_ab = xgb.DMatrix(X_tr[cols_sin_recontacto], label=y_tr, weight=sw)\n",
        "dvalid_ab = xgb.DMatrix(X_val[cols_sin_recontacto], label=y_val)\n",
        "dtest_ab  = xgb.DMatrix(X_test_ext[cols_sin_recontacto])\n",
        "\n",
        "booster_ab = xgb.train(params, dtrain_ab, num_boost_round=2000,\n",
        "                       evals=[(dtrain_ab,'train'),(dvalid_ab,'valid')],\n",
        "                       early_stopping_rounds=50, verbose_eval=False)\n",
        "\n",
        "y_proba_ab = booster_ab.predict(dtest_ab, iteration_range=(0, booster_ab.best_iteration+1))\n",
        "y_pred_ab  = (y_proba_ab >= 0.58).astype(int)  # usa tu umbral final para comparar\n",
        "print(classification_report(y_test, y_pred_ab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7431468",
      "metadata": {
        "id": "e7431468"
      },
      "outputs": [],
      "source": [
        "# === Explicabilidad con SHAP ===\n",
        "# Importancia global (bar/beeswarm) y dependence plots para variables top.\n",
        "# Importancias (ya viste el top10 por 'gain')\n",
        "imp = booster.get_score(importance_type='gain')\n",
        "print(sorted(imp.items(), key=lambda x: -x[1])[:10])\n",
        "\n",
        "# (Opcional) SHAP rápido\n",
        "# !pip install shap\n",
        "import shap\n",
        "explainer = shap.TreeExplainer(booster)\n",
        "shap_values = explainer.shap_values(xgb.DMatrix(X_test_ext))\n",
        "shap.summary_plot(shap_values, X_test_ext, show=False)  # en Colab, luego plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d0d243",
      "metadata": {
        "id": "73d0d243"
      },
      "outputs": [],
      "source": [
        "# ===== Reentrenar XGB nativo SIN Recontacto_sf =====\n",
        "import numpy as np, xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, f1_score, recall_score\n",
        "\n",
        "# 1) Subset de features SIN Recontacto_sf\n",
        "cols_sin_recontacto = [c for c in X_train_ext.columns if c != 'Recontacto_sf']\n",
        "X_tr_full = X_train_ext[cols_sin_recontacto]\n",
        "X_te_full = X_test_ext[cols_sin_recontacto]\n",
        "\n",
        "# 2) Split interno p/ early stopping\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_tr_full, y_train, test_size=0.10, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "# 3) Pesos favoreciendo clase 0\n",
        "ratio = (y_tr==1).sum() / max(1,(y_tr==0).sum())\n",
        "sw = np.where(y_tr==0, ratio, 1.0)\n",
        "\n",
        "# 4) DMatrix\n",
        "dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=sw)\n",
        "dvalid = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest  = xgb.DMatrix(X_te_full)\n",
        "\n",
        "# 5) Parámetros (mismos que venías usando)\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'max_depth': 5,\n",
        "    'eta': 0.05,\n",
        "    'subsample': 0.9,\n",
        "    'colsample_bytree': 0.9,\n",
        "    'min_child_weight': 2,\n",
        "    'lambda': 1.0,\n",
        "    'alpha': 0.5,\n",
        "    'tree_method': 'hist',\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "booster_sin = xgb.train(\n",
        "    params=params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=2000,\n",
        "    evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=False\n",
        ")\n",
        "print(f\"Mejor iteración (sin Recontacto): {booster_sin.best_iteration} | logloss(valid): {booster_sin.best_score:.4f}\")\n",
        "\n",
        "# 6) Predicciones test\n",
        "y_proba_sin = booster_sin.predict(dtest, iteration_range=(0, booster_sin.best_iteration+1))\n",
        "\n",
        "# Report con umbral 0.50 (referencia)\n",
        "y_pred050 = (y_proba_sin >= 0.50).astype(int)\n",
        "print(\"📌 Report (0.50) sin Recontacto:\")\n",
        "print(classification_report(y_test, y_pred050))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_sin))\n",
        "\n",
        "# 7) Elegir umbral para recall_0 ≥ 0.70 (o tu objetivo)\n",
        "def umbral_por_objetivo(y_true, proba_pos, min_recall0=0.70):\n",
        "    best = {'thr':0.5, 'rec0':0, 'f1_0':-1}\n",
        "    for thr in np.linspace(0.1, 0.9, 81):\n",
        "        y_hat = (proba_pos >= thr).astype(int)\n",
        "        r0 = recall_score(y_true, y_hat, pos_label=0)\n",
        "        f10 = f1_score(y_true, y_hat, pos_label=0)\n",
        "        if r0 >= min_recall0 and f10 > best['f1_0']:\n",
        "            best = {'thr':thr, 'rec0':r0, 'f1_0':f10}\n",
        "    return best\n",
        "\n",
        "best_obj_sin = umbral_por_objetivo(y_test, y_proba_sin, min_recall0=0.70)\n",
        "print(\"🎯 Umbral (sin Recontacto) con recall0 ≥ 0.70:\", best_obj_sin)\n",
        "\n",
        "y_hat_obj_sin = (y_proba_sin >= best_obj_sin['thr']).astype(int)\n",
        "print(\"📌 Report (umbral objetivo) sin Recontacto:\")\n",
        "print(classification_report(y_test, y_hat_obj_sin))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55efa44",
      "metadata": {
        "id": "d55efa44"
      },
      "outputs": [],
      "source": [
        "# === Bloque misceláneo ===\n",
        "bins = [-0.1,6,12,24,60,1e9]; labels = ['<6m','6-12m','12-24m','2-5y','5y+']\n",
        "df['antiguedad_bucket'] = pd.cut(df['antiguedad_cliente_meses'], bins=bins, labels=labels)\n",
        "# añadila a whitelist, recodificá, rearmá X_train/X_test y vuelve a crear *_freq/_mean0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d7fedc",
      "metadata": {
        "id": "89d7fedc"
      },
      "outputs": [],
      "source": [
        "# === Limpieza / Normalización: manejo de nulos, tipos, merges y depuración ===\n",
        "# Objetivo: dejar los datos consistentes antes del EDA/modelado.\n",
        "# Guardar modelo, umbral y listado de features\n",
        "booster_sin.save_model(\"xgb_csat_sin_recontacto.json\")\n",
        "\n",
        "umbral_final = 0.56   # o 0.58, el que elijas\n",
        "np.save(\"umbral_final.npy\", np.array([umbral_final]))\n",
        "\n",
        "cols_sin_recontacto = [c for c in X_train_ext.columns if c != 'Recontacto_sf']\n",
        "with open(\"features_usadas.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(cols_sin_recontacto))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73e9217",
      "metadata": {
        "id": "e73e9217",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "# ----------------------------\n",
        "# 1) Asegurar columna de MES\n",
        "# ----------------------------\n",
        "# Usaremos 'mes_encuesta' si ya existe. Si no, la creamos desde alguna fecha disponible.\n",
        "if 'mes_encuesta' not in df.columns:\n",
        "    # Intentar a partir de fecha_encuesta o columnas similares\n",
        "    posibles = ['Fecha Registrada corta','Fecha registrada']\n",
        "    col_fecha = next((c for c in posibles if c in df.columns), None)\n",
        "    if col_fecha is None:\n",
        "        raise ValueError(\"No encuentro una columna de fecha para inferir el mes.\")\n",
        "    df[col_fecha] = pd.to_datetime(df[col_fecha], errors='coerce')\n",
        "    df['mes_encuesta'] = df[col_fecha].dt.month\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Reconstruir df_modelo limpio (whitelist)\n",
        "# ----------------------------\n",
        "whitelist = [\n",
        "    'antiguedad_cliente_meses','rango_etario','genero','ya_es_cliente','ciudad',\n",
        "    'Producto_sf','Subproduct_sf','grupo_producto','motivo_contacto_categoria',\n",
        "    'Canal','canal_ticket','Duración (en segundos)','mes_encuesta','dia_semana_encuesta','hora_encuesta'\n",
        "]\n",
        "cols_exist = [c for c in whitelist if c in df.columns]\n",
        "df_modelo = df[cols_exist + ['CSAT_bin']].copy()\n",
        "\n",
        "# Anti-fuga (por si se coló algo de CSAT o Resolución textual)\n",
        "ban_patterns = ['csat','satisf','resoluc']\n",
        "bad = [c for c in df_modelo.columns if any(p in c.lower() for p in ban_patterns) and c!='CSAT_bin']\n",
        "if bad:\n",
        "    df_modelo = df_modelo.drop(columns=bad)\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Codificar categóricas SOLO en object/category\n",
        "# ----------------------------\n",
        "X_raw = df_modelo.drop(columns=['CSAT_bin']).copy()\n",
        "y_all = df_modelo['CSAT_bin'].astype(int)\n",
        "for col in X_raw.select_dtypes(include=['object','category']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X_raw[col] = le.fit_transform(X_raw[col].astype(str))\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Imputación (fit en TRAIN temporal)\n",
        "# ----------------------------\n",
        "# Definir máscaras temporales\n",
        "mask_train = df['mes_encuesta'].isin([2,3,4])  # Feb-Abr\n",
        "mask_test  = df['mes_encuesta'].isin([5,6])    # May-Jun\n",
        "\n",
        "imp = SimpleImputer(strategy='mean')\n",
        "X_train_ts = pd.DataFrame(imp.fit_transform(X_raw[mask_train]), columns=X_raw.columns, index=X_raw[mask_train].index)\n",
        "X_test_ts  = pd.DataFrame(imp.transform(X_raw[mask_test]),  columns=X_raw.columns, index=X_raw[mask_test].index)\n",
        "y_train_ts = y_all[mask_train]\n",
        "y_test_ts  = y_all[mask_test]\n",
        "\n",
        "print(\"Temporal shapes:\", X_train_ts.shape, X_test_ts.shape)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Encodings adicionales: *_freq y *_mean0 (fit SOLO en train temporal)\n",
        "# ----------------------------\n",
        "cand_cols = [c for c in [\n",
        "    'Producto_sf','Subproduct_sf','grupo_producto','motivo_contacto_categoria',\n",
        "    'rango_etario','genero','ya_es_cliente','ciudad','Canal','canal_ticket'\n",
        "] if c in X_train_ts.columns]\n",
        "\n",
        "def add_freq_and_target_mean(Xtr, Xte, ytr, cols, positive_class=0):\n",
        "    Xtr = Xtr.copy(); Xte = Xte.copy()\n",
        "    base_p0 = (ytr == positive_class).mean()\n",
        "    for col in cols:\n",
        "        # frequency\n",
        "        freq = Xtr[col].value_counts(normalize=True)\n",
        "        Xtr[f'{col}_freq'] = Xtr[col].map(freq)\n",
        "        Xte[f'{col}_freq'] = Xte[col].map(freq).fillna(0)\n",
        "        # target-mean (prob de clase 0)\n",
        "        tmp = pd.DataFrame({col: Xtr[col], 'y': ytr.values})\n",
        "        mean1 = tmp.groupby(col)['y'].mean()\n",
        "        prob0 = 1.0 - mean1\n",
        "        Xtr[f'{col}_mean0'] = Xtr[col].map(prob0)\n",
        "        Xte[f'{col}_mean0'] = Xte[col].map(prob0).fillna(base_p0)\n",
        "    return Xtr, Xte\n",
        "\n",
        "X_tr_ext, X_te_ext = add_freq_and_target_mean(X_train_ts, X_test_ts, y_train_ts, cand_cols, positive_class=0)\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Entrenar XGB nativo con early stopping (solo TRAIN temporal)\n",
        "# ----------------------------\n",
        "import xgboost as xgb\n",
        "ratio = (y_train_ts==1).sum() / max(1,(y_train_ts==0).sum())\n",
        "sw = np.where(y_train_ts==0, ratio, 1.0)\n",
        "\n",
        "# split interno para early stopping dentro del TRAIN temporal\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_tr_ext, y_train_ts, test_size=0.10, random_state=42, stratify=y_train_ts)\n",
        "\n",
        "dtrain = xgb.DMatrix(X_tr,  label=y_tr,  weight=np.where(y_tr==0, ratio, 1.0))\n",
        "dvalid = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest  = xgb.DMatrix(X_te_ext)\n",
        "\n",
        "params = {\n",
        "    'objective':'binary:logistic','eval_metric':'logloss',\n",
        "    'max_depth':5,'eta':0.05,'subsample':0.9,'colsample_bytree':0.9,\n",
        "    'min_child_weight':2,'lambda':1.0,'alpha':0.5,'tree_method':'hist','seed':42\n",
        "}\n",
        "booster_ts = xgb.train(params, dtrain, num_boost_round=2000,\n",
        "                       evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "                       early_stopping_rounds=50, verbose_eval=False)\n",
        "\n",
        "print(f\"Mejor iteración (temporal): {booster_ts.best_iteration} | logloss(valid): {booster_ts.best_score:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Evaluación en TEST temporal (May-Jun)\n",
        "# ----------------------------\n",
        "y_proba_ts = booster_ts.predict(dtest, iteration_range=(0, booster_ts.best_iteration+1))\n",
        "y_pred050  = (y_proba_ts >= 0.50).astype(int)\n",
        "print(\"📌 Report temporal (umbral 0.50):\")\n",
        "print(classification_report(y_test_ts, y_pred050))\n",
        "print(\"ROC-AUC temporal:\", roc_auc_score(y_test_ts, y_proba_ts))\n",
        "\n",
        "# Umbral por objetivo (ej: recall0 ≥ 0.70)\n",
        "def umbral_por_objetivo(y_true, proba_pos, min_recall0=0.70):\n",
        "    best = {'thr':0.5, 'rec0':0, 'f1_0':-1}\n",
        "    for thr in np.linspace(0.1,0.9,81):\n",
        "        y_hat = (proba_pos >= thr).astype(int)\n",
        "        r0 = recall_score(y_true, y_hat, pos_label=0)\n",
        "        f10 = f1_score(y_true, y_hat, pos_label=0)\n",
        "        if r0 >= min_recall0 and f10 > best['f1_0']:\n",
        "            best = {'thr':thr, 'rec0':r0, 'f1_0':f10}\n",
        "    return best\n",
        "\n",
        "best_obj_ts = umbral_por_objetivo(y_test_ts, y_proba_ts, min_recall0=0.70)\n",
        "print(\"🎯 Umbral temporal con recall0 ≥ 0.70:\", best_obj_ts)\n",
        "\n",
        "y_hat_ts = (y_proba_ts >= best_obj_ts['thr']).astype(int)\n",
        "print(\"📌 Report temporal (umbral objetivo):\")\n",
        "print(classification_report(y_test_ts, y_hat_ts))\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test_ts, y_hat_ts), annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=['No Sat','Sat'], yticklabels=['No Sat','Sat'])\n",
        "plt.title(\"Matriz — Validación temporal (umbral objetivo)\")\n",
        "plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e184fd28",
      "metadata": {
        "id": "e184fd28"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "# Modelo (entrenado en Feb–Abr) + umbral encontrado para May–Jun\n",
        "booster_ts.save_model(\"xgb_csat_temporal_febabr.json\")\n",
        "\n",
        "import numpy as np\n",
        "np.save(\"umbral_temporal.npy\", np.array([float(best_obj_ts['thr'])]))  # ej: 0.66\n",
        "\n",
        "# Columnas usadas y su orden\n",
        "with open(\"features_temporal.txt\",\"w\") as f:\n",
        "    f.write(\"\\n\".join(X_tr_ext.columns.astype(str)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4689f14d",
      "metadata": {
        "id": "4689f14d"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def confusion_clase0(y_true, y_hat):\n",
        "    # tratamos \"0\" como la clase positiva\n",
        "    y_true0 = (y_true == 0).astype(int)\n",
        "    y_hat0  = (y_hat  == 0).astype(int)\n",
        "    tn0, fp0, fn0, tp0 = confusion_matrix(y_true0, y_hat0).ravel()\n",
        "    return tn0, fp0, fn0, tp0  # tp0 = aciertos \"No Sat\"; fn0 = \"No Sat\" perdidos\n",
        "\n",
        "def umbral_por_coste_clase0(y_true, proba_pos, cost_fn=5.0, cost_fp=1.0):\n",
        "    mejores = {'thr':0.5, 'costo':float('inf'), 'tn0':0,'fp0':0,'fn0':0,'tp0':0}\n",
        "    import numpy as np\n",
        "    for thr in np.linspace(0.1, 0.9, 81):\n",
        "        y_hat = (proba_pos >= thr).astype(int)  # 1=\"Sat\", 0=\"No Sat\"\n",
        "        tn0, fp0, fn0, tp0 = confusion_clase0(y_true, y_hat)\n",
        "        costo = cost_fn*fn0 + cost_fp*fp0\n",
        "        if costo < mejores['costo']:\n",
        "            mejores = {'thr':float(thr), 'costo':float(costo),\n",
        "                       'tn0':int(tn0),'fp0':int(fp0),'fn0':int(fn0),'tp0':int(tp0)}\n",
        "    return mejores\n",
        "\n",
        "best_cost0 = umbral_por_coste_clase0(y_test_ts, y_proba_ts, cost_fn=5.0, cost_fp=1.0)\n",
        "best_cost0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da03ce9",
      "metadata": {
        "id": "8da03ce9"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "from sklearn.metrics import fbeta_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def umbral_por_Fbeta_clase0(y_true, proba_pos, beta=2.0):\n",
        "    mejor = {'thr':0.5,'fbeta_0':-1,'rec0':0,'prec0':0}\n",
        "    for thr in np.linspace(0.1, 0.9, 81):\n",
        "        y_hat = (proba_pos >= thr).astype(int)\n",
        "        # calculamos F_beta tratando \"0\" como la clase de interés\n",
        "        y_true0 = (y_true == 0).astype(int)\n",
        "        y_hat0  = (y_hat  == 0).astype(int)\n",
        "        f0 = fbeta_score(y_true0, y_hat0, beta=beta)\n",
        "        r0 = recall_score(y_true0, y_hat0)\n",
        "        p0 = precision_score(y_true0, y_hat0, zero_division=0)\n",
        "        if f0 > mejor['fbeta_0']:\n",
        "            mejor = {'thr':float(thr),'fbeta_0':float(f0),'rec0':float(r0),'prec0':float(p0)}\n",
        "    return mejor\n",
        "\n",
        "best_f2_ts = umbral_por_Fbeta_clase0(y_test_ts, y_proba_ts, beta=2.0)\n",
        "best_f2_ts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118324d0",
      "metadata": {
        "id": "118324d0"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "import pandas as pd\n",
        "\n",
        "def comparar_proporciones(df, col, mask_train, mask_test, top_n=10):\n",
        "    p_train = df.loc[mask_train, col].value_counts(normalize=True)\n",
        "    p_test  = df.loc[mask_test,  col].value_counts(normalize=True)\n",
        "    comp = pd.concat([p_train.rename('train'), p_test.rename('test')], axis=1).fillna(0)\n",
        "    comp['delta_pp'] = (comp['test'] - comp['train'])*100\n",
        "    return comp.reindex(comp['delta_pp'].abs().sort_values(ascending=False).index).head(top_n)\n",
        "\n",
        "# Ejemplos (usa columnas que tengas)\n",
        "cols_cat = [c for c in ['motivo_contacto_categoria','Producto_sf','Subproduct_sf','Canal','ciudad'] if c in df.columns]\n",
        "\n",
        "for c in cols_cat:\n",
        "    print(f\"\\n>>> Drift top en {c}\")\n",
        "    display(comparar_proporciones(df, c, mask_train, mask_test, top_n=10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a289e1f0",
      "metadata": {
        "id": "a289e1f0"
      },
      "outputs": [],
      "source": [
        "# === Limpieza / Normalización: manejo de nulos, tipos, merges y depuración ===\n",
        "# Objetivo: dejar los datos consistentes antes del EDA/modelado.\n",
        "\n",
        "def confusion_clase0(y_true, y_hat):\n",
        "    y_true0 = (y_true == 0).astype(int)\n",
        "    y_hat0  = (y_hat  == 0).astype(int)\n",
        "    tn0, fp0, fn0, tp0 = confusion_matrix(y_true0, y_hat0).ravel()\n",
        "    return tn0, fp0, fn0, tp0\n",
        "\n",
        "def resumen_umbral(y_true, proba_pos, thr, cost_fn=5.0, cost_fp=1.0):\n",
        "    y_hat = (proba_pos >= thr).astype(int)    # 1=Sat, 0=No Sat\n",
        "    tn0, fp0, fn0, tp0 = confusion_clase0(y_true, y_hat)\n",
        "    prec0 = tp0 / (tp0 + fp0 + 1e-9)\n",
        "    rec0  = tp0 / (tp0 + fn0 + 1e-9)\n",
        "    f1_0  = 2*prec0*rec0 / (prec0 + rec0 + 1e-9)\n",
        "    costo = cost_fn*fn0 + cost_fp*fp0\n",
        "    return {\n",
        "        'thr': thr, 'prec0': prec0, 'rec0': rec0, 'f1_0': f1_0,\n",
        "        'fp0': int(fp0), 'fn0': int(fn0), 'tp0': int(tp0), 'tn0': int(tn0), 'costo': float(costo)\n",
        "    }\n",
        "\n",
        "umbrales = [0.50, 0.66, 0.73, 0.88]\n",
        "tabla = [resumen_umbral(y_test_ts, y_proba_ts, t, cost_fn=5.0, cost_fp=1.0) for t in umbrales]\n",
        "for row in tabla:\n",
        "    print(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142b2a28",
      "metadata": {
        "id": "142b2a28"
      },
      "outputs": [],
      "source": [
        "# === Bloque misceláneo ===\n",
        "# --- Importancias (gain) top 15 ---\n",
        "imp = booster_ts.get_score(importance_type='gain')\n",
        "top = sorted(imp.items(), key=lambda x: -x[1])[:15]\n",
        "for k,v in top:\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f89e8b4",
      "metadata": {
        "id": "9f89e8b4"
      },
      "outputs": [],
      "source": [
        "# === Explicabilidad con SHAP ===\n",
        "# Importancia global (bar/beeswarm) y dependence plots para variables top.\n",
        "# --- SHAP summary plot ---\n",
        "# !pip install shap\n",
        "\n",
        "# Usamos el mismo booster_ts y X_te_ext de la validación temporal\n",
        "explainer = shap.TreeExplainer(booster_ts)\n",
        "# OJO: SHAP espera matriz/array, pasamos el mismo orden de columnas\n",
        "X_plot = X_te_ext.copy()\n",
        "shap_values = explainer.shap_values(xgb.DMatrix(X_plot))\n",
        "\n",
        "shap.summary_plot(shap_values, X_plot, show=False)\n",
        "plt.title(\"SHAP Summary — Test temporal (May–Jun)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce29c5d4",
      "metadata": {
        "id": "ce29c5d4"
      },
      "outputs": [],
      "source": [
        "# === Limpieza / Normalización: manejo de nulos, tipos, merges y depuración ===\n",
        "# Objetivo: dejar los datos consistentes antes del EDA/modelado.\n",
        "def confusion_clase0(y_true, y_hat):\n",
        "    y_true0=(y_true==0).astype(int); y_hat0=(y_hat==0).astype(int)\n",
        "    tn0, fp0, fn0, tp0 = confusion_matrix(y_true0, y_hat0).ravel()\n",
        "    return tn0, fp0, fn0, tp0\n",
        "\n",
        "def fila(y_true, proba_pos, thr, cost_fn=5.0, cost_fp=1.0):\n",
        "    y_hat = (proba_pos >= thr).astype(int)\n",
        "    tn0, fp0, fn0, tp0 = confusion_clase0(y_true, y_hat)\n",
        "    prec0 = tp0/(tp0+fp0+1e-9); rec0 = tp0/(tp0+fn0+1e-9)\n",
        "    f1_0 = 2*prec0*rec0/(prec0+rec0+1e-9)\n",
        "    costo = cost_fn*fn0 + cost_fp*fp0\n",
        "    return [thr, prec0, rec0, f1_0, fp0, fn0, costo]\n",
        "\n",
        "df_tab = pd.DataFrame(\n",
        "    [fila(y_test_ts, y_proba_ts, t) for t in [0.50, 0.66, 0.73, 0.88]],\n",
        "    columns=[\"Umbral\",\"Prec_0\",\"Recall_0\",\"F1_0\",\"FP_0\",\"FN_0\",\"Costo(5*FN0+FP0)\"]\n",
        ").round(3)\n",
        "df_tab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e82307",
      "metadata": {
        "id": "93e82307"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "#import matplotlib.pyplot as plt, numpy as np\n",
        "\n",
        "y0 = (y_test_ts==0).astype(int)          # clase positiva = No Sat\n",
        "score0 = 1 - y_proba_ts                  # prob(No Sat)\n",
        "\n",
        "# ROC\n",
        "fpr, tpr, _ = roc_curve(y0, score0)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr); plt.plot([0,1],[0,1],'--')\n",
        "plt.title(f\"ROC (No Sat) - AUC={roc_auc:.3f}\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.show()\n",
        "\n",
        "# Precision-Recall\n",
        "prec, rec, thr = precision_recall_curve(y0, score0)\n",
        "plt.plot(rec, prec); plt.title(\"Precision-Recall (No Sat)\")\n",
        "plt.xlabel(\"Recall₀\"); plt.ylabel(\"Precision₀\")\n",
        "# marca tus umbrales (pSat→pNoSat = 1-umbral)\n",
        "for t in [0.66, 0.73]:\n",
        "    t0 = 1 - t\n",
        "    j = np.argmin(np.abs(thr - t0))\n",
        "    plt.scatter(rec[j], prec[j], label=f\"thr={t}\", s=60)\n",
        "plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab10abc8",
      "metadata": {
        "id": "ab10abc8"
      },
      "outputs": [],
      "source": [
        "# === Importaciones: librerías usadas en todo el notebook ===\n",
        "#import seaborn as sns, pandas as pd\n",
        "dfp = pd.DataFrame({\"pSat\": y_proba_ts, \"y\": y_test_ts.map({0:\"No Sat\",1:\"Sat\"})})\n",
        "sns.kdeplot(data=dfp, x=\"pSat\", hue=\"y\", common_norm=False);\n",
        "for t in [0.66,0.73]: plt.axvline(t, ls='--', alpha=.4)\n",
        "plt.title(\"Distribución p(Sat) por clase\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acab99b7",
      "metadata": {
        "id": "acab99b7"
      },
      "outputs": [],
      "source": [
        "# === Explicabilidad con SHAP ===\n",
        "# Importancia global (bar/beeswarm) y dependence plots para variables top.\n",
        "# summary tipo barra (importancia media |SHAP|)\n",
        "shap.summary_plot(shap_values, X_te_ext, plot_type=\"bar\", show=False)\n",
        "plt.title(\"SHAP Bar — Test temporal\"); plt.show()\n",
        "\n",
        "# dependence plots (relación de 1 variable con el output)\n",
        "for feat in [\"Subproduct_sf_mean0\",\"Duración (en segundos)\",\"Producto_sf_mean0\"]:\n",
        "    shap.dependence_plot(feat, shap_values, X_te_ext, show=False)\n",
        "    plt.title(f\"SHAP Dependence — {feat}\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "534805ef",
      "metadata": {
        "id": "534805ef"
      },
      "outputs": [],
      "source": [
        "# === Limpieza / Normalización: manejo de nulos, tipos, merges y depuración ===\n",
        "# Objetivo: dejar los datos consistentes antes del EDA/modelado.\n",
        "y0 = (y_test_ts==0).astype(int)\n",
        "score0 = 1 - y_proba_ts\n",
        "order = np.argsort(-score0)              # mayor riesgo primero\n",
        "y0_sorted = y0.iloc[order].to_numpy()\n",
        "\n",
        "cum_pos = np.cumsum(y0_sorted)\n",
        "total_pos = y0.sum()\n",
        "pct_pobl = np.arange(1, len(y0_sorted)+1)/len(y0_sorted)\n",
        "rec_cum  = cum_pos/total_pos\n",
        "\n",
        "plt.plot(pct_pobl, rec_cum, label=\"Modelo\")\n",
        "plt.plot(pct_pobl, pct_pobl, '--', label=\"Aleatorio\")\n",
        "plt.xlabel(\"% población ordenada por riesgo\"); plt.ylabel(\"Recall₀ acumulado\")\n",
        "plt.title(\"Cumulative Gains — No Sat\"); plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "a0344556",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0344556",
        "outputId": "13a6f82f-54b1-4c38-d632-3abef3594155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "-rw------- 1 root root 1575653 Sep 18 05:10 /content/drive/MyDrive/TPF/TFM_CSAT_Desde_Drive.ipynb\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/TPF/TFM_CSAT_Desde_Drive.ipynb to html\n",
            "[NbConvertApp] WARNING | Alternative text is missing on 22 image(s).\n",
            "[NbConvertApp] Writing 2006813 bytes to /content/drive/MyDrive/TPF/TFM_CSAT2.html\n"
          ]
        }
      ],
      "source": [
        "# === Montaje de Google Drive para acceder/guardar archivos ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#!ls \"/content/drive/MyDrive/TPF\"\n",
        "\n",
        "!ls -l \"/content/drive/MyDrive/TPF/TFM_CSAT_Desde_Drive.ipynb\"\n",
        "\n",
        "#!jupyter nbconvert --to html \"/content/drive/MyDrive/TFM_CSAT_Desde_Drive.ipynb\" --output \"/content/drive/MyDrive/TFM_CSAT.html\"\n",
        "\n",
        "!jupyter nbconvert --to html \"/content/drive/MyDrive/TPF/TFM_CSAT_Desde_Drive.ipynb\" --output \"/content/drive/MyDrive/TPF/TFM_CSAT2.html\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}